# -*- coding: utf-8 -*-
"""Loan-or-Not-dataPreprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Xw7Ujc87Wwu-mXmci2P4_CNWmvNK3y7S
"""

# importing the essential packages
import array
import numpy as np
import json
import pandas as pd
from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.metrics import r2_score
from sklearn.svm import LinearSVR
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import cross_val_score
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt
import seaborn as sns
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers
from sklearn import preprocessing
from keras import backend as K
from re import X

# to retirve and manipulate files which are stored in the google drive
from google.colab import drive
drive.mount('/content/drive')

from pydrive.auth import GoogleAuth
from pydrive.drive import GoogleDrive 
from google.colab import auth 
from oauth2client.client import GoogleCredentials

auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()                       
drive = GoogleDrive(gauth)

# Reading the given csv file as Data Frame
df = pd.read_csv("drive/My Drive/Stout/loans_full_schema.csv")

# Viewing the Data Frame's first 10 rows
df.head(10)

# Seeing the count of values for each unique entry in the Data Frame
df.value_counts()

# Viewing the shape of the data frame
df.shape

# Printing the name of the columns of the Data Frame
print(df.columns)

# Summarizing the information in the Data Frame
df.info()

# Viewing the numerical statictics for the numerical data  
df.describe()

# Viewing the categorical statictics for the categorical data 
df.describe(include=["object"])

# Dividing the df dataframe into x and y
# where x : features for prediction and y : Value (interest rate) to be predicted

x = df.loc[:, df.columns != 'interest_rate']
y = df['interest_rate']

# Splitting the data x in train and validation sets and defining a test set too
x_train, x_test, y_train, y_test = train_test_split(x, y, train_size=0.8, shuffle=True)

# Viewing the shapes of x_train, x_test, y_train and y_test
print(x_train.shape)
print(x_test.shape)
print(y_train.shape)
print(y_test.shape)

"""# **Data Pre-processing**
The data is pre-processed before feeding it into the predictive models. The following techniques have been used for carrying out the data pre-processing:


1.   **Missing values** are found and taken care of. We find the % of missing values in every column and remove columns which have greater than 75% missing in them. We further fill 'unemployed' in place of missing values in the emp_title column after checking a certain condition. At last, the missing values in the remaining columns are filled with the mode value for the column.
2.   The datatypes of the data are found and the column names are segregated into 'categorical' and 'numerical'. **Normalization** using MinMaxScaler() is performed on the numerical training and test data.
3.   Next, the categoricals are mutated to only keep the entries which are the top 25 of the daframe otherwise they are changed to 'other'.
4.   Label encoding is performed on the training and test data.
5.   Correlation coefficient between the columns are calculated and the highly correlated columns are dropped.



"""

# Finding missing values by column names and percentage in x_train and x_test sets
train_missing = (x_train.isnull().sum() / len(x_train)).sort_values(ascending = False)
test_missing = (x_test.isnull().sum() / len(x_test)).sort_values(ascending = False)

# Printing the missing values by percentage in the training set
train_missing

# Keeping 75% as threshold.
# Only keeping the missing values greater than 75% in the test_missing
train_missing = train_missing.index[train_missing > 0.75]
train_missing

# Keeping 75% as threshold.
# Only keeping the missing values greater than 75% in the test_missing
test_missing = test_missing.index[test_missing > 0.75]
test_missing

# Finding common missing values in missing training and test set
all_missing = list(set(set(train_missing) | set(test_missing)))

# Printing the common missing values
all_missing

# Dropping those common missing values from train and test set
x_train.drop(columns = all_missing, axis=1, inplace=True)
x_test.drop(columns = all_missing, axis=1, inplace=True)

# Checking if the missing values in emp_title means unemployed ot other
unemployed = ['unemployed', 'none', 'Unemployed', 'other', 'Other']
for item in unemployed:
    if item in df['emp_title']:
        print("Found It at ", item)

# Handle the missing values induvidually per column for training set
x_train['emp_title'] = x_train['emp_title'].fillna("Unemployed")
mode_cols = ['months_since_last_delinq', 'months_since_last_credit_inquiry', 'num_accounts_120d_past_due', 'emp_length', 'debt_to_income']

for col in mode_cols:
  x_train[col] = x_train[col].fillna(x_train[col].mode()[0])

# Handle the missing values induvidually per column for test set
x_test['emp_title'] = x_test['emp_title'].fillna("Unemployed")
mode_cols = ['months_since_last_delinq', 'months_since_last_credit_inquiry', 'num_accounts_120d_past_due', 'emp_length', 'debt_to_income']

for col in mode_cols:
  x_test[col] = x_test[col].fillna(x_test[col].mode()[0])

# Checking is training set contains any missing value; False -> No missing value
any(x_train.isna().any())

# Checking is test set contains any missing value; False -> No missing value
any(x_test.isna().any())

# Viewing the details of x_train
x_train.info()

# Segregating data into data types for further pre-processing
categorical = x_train.select_dtypes(include=['object']).columns.tolist()
numerical = x_train.select_dtypes(exclude=['object']).columns.tolist()

# Viewing the items of categorical data type
categorical

# Viewing the items of numerical data type
numerical

# Normalization of x train and x test
x_train[numerical] = MinMaxScaler().fit_transform(x_train[numerical])
x_test[numerical] = MinMaxScaler().fit_transform(x_test[numerical])

# Normalizing the labels 
y_train = MinMaxScaler().fit_transform(np.reshape(y_train.values, (-1,1)))
y_test = MinMaxScaler().fit_transform(np.reshape(y_test.values, (-1,1)))

# Mutating the categoricals to only keep the entries which are the top 25 of the daframe otherwise they become other
for c in categorical:
  tops = x_train[c].value_counts().index[:25]
  def helper(x):
    if x in tops:
      return x
    else:
      return "Other"

  x_train[c] = x_train[c].apply(helper)
  x_test[c] = x_test[c].apply(helper)

# Viewing the unique values of x_train 's emp_title 
x_train['emp_title'].value_counts()

# Viewing the unique values of x_train 's grade
x_train['grade'].value_counts()

# Viewing the unique values of x_train 's verified income
x_train['verified_income'].value_counts()

# Label encoder
for category in categorical:
  le = LabelEncoder()
  x_train[category] = le.fit_transform(x_train[category])
  x_test[category] = le.transform(x_test[category])

# Using Pearson's Correlation on training set and visualizing using a heatmap
plt.figure(figsize=(12,10))
cor = x_train.corr()
sns.heatmap(cor, annot=True, cmap=plt.cm.Reds)
plt.show()

# Setting the correlation threshold = 0.9
threshold = 0.9

# Absolute value correlation matrix
corr_matrix = cor.abs()

# Viewing the correlation matrix
corr_matrix.head()

# Finding the upper bound of correlation value
upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))
upper.head()

# Finding the high correlation value columns 
to_drop = [column for column in upper.columns if any(upper[column] > threshold)]

print('There are %d columns to remove.' % (len(to_drop)))

# Dropping the high correlation value columns from the training and test set
x_train = x_train.drop(columns = to_drop)
x_test = x_test.drop(columns = to_drop)

print('Training shape: ', x_train.shape)
print('Testing shape: ', x_test.shape)

# Predictive Modelling
# Decision Tree

regressor = DecisionTreeRegressor(random_state=0, max_depth = 10)
regressor.fit(x_train, y_train)
y_pred_regressor = regressor.predict(x_train)

# Calculating R2 score of prediction on training data
r2_regressor = r2_score(y_train, y_pred_regressor)
print(r2_regressor)

# Prediction on test data
y_pred_regressor = regressor.predict(x_test)

# Calculating R2 score of prediction on test data
r2_regressor = r2_score(y_test, y_pred_regressor)
print(r2_regressor)

# Random Forest
random_forest_regressor = RandomForestRegressor(n_estimators = 50, random_state = 0, verbose = 10)
random_forest_regressor.fit(x_train, y_train)

# Calculating R2 score of prediction on training data
y_pred_r = random_forest_regressor.predict(x_train)
r2_rf = r2_score(y_train, y_pred_r)
print(r2_rf)

# Calculating R2 score of prediction on test data
y_pred_test_r = random_forest_regressor.predict(x_test)
r2_rf_test = r2_score(y_test, y_pred_test_r)
print(r2_rf_test)

# Neural network

# Function to calculate R2 score or coeff of determination
def coeff_determination(y_true, y_pred):
    SS_res =  K.sum(K.square( y_true-y_pred ))
    SS_tot = K.sum(K.square( y_true - K.mean(y_true) ) )
    return ( 1 - SS_res/(SS_tot + K.epsilon()) )

# The model used for predictions
model = keras.Sequential([
                          layers.Dense(32, activation='relu'),
                          layers.Dense(64, activation='relu'),
                          layers.Dense(128, activation='relu'),
                          layers.Dense(512, activation='relu'),
                          layers.Dense(128, activation='relu'),
                          layers.Dense(64, activation='relu'),
                          layers.Dense(32, activation='relu'),
                          layers.Dense(1)
                          ])
model.compile(loss='mean_squared_error',optimizer=tf.keras.optimizers.Adam(0.01), metrics = [coeff_determination])

history = model.fit(
    x_train,
    y_train,
    epochs=50,
    validation_split= 0.2)

# Prediction on training data
train_pred = model.predict(x_train).flatten()
print(train_pred)

# Scatter plot for Predictions and True Values of training data
a = plt.axes(aspect='equal')
plt.scatter(y_train, train_pred)
plt.xlabel('True Values')
plt.ylabel('Predictions')
lims = [0, 1]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims, lims)

# Graph for Training Loss vs Validation Loss of training data
plt.plot(history.history['loss'], label='loss')
plt.plot(history.history['val_loss'], label='val_loss')
plt.ylim([0, 0.02])
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

# Graph for Training coeff of determination vs Validation coeff of determination of training data
plt.plot(history.history['coeff_determination'], label='training R2')
plt.plot(history.history['val_coeff_determination'], label='validation R2')
plt.ylim([0, 0.99])
plt.xlabel('Epoch')
plt.ylabel('R2 score')
plt.legend()
plt.grid(True)

# Predictions on Testing Data
test_pred = model.predict(x_test).flatten()
print(train_pred)

# Scatter plot for Predictions and True Values of test data
b = plt.axes(aspect='equal')
plt.scatter(y_test, test_pred)
plt.xlabel('True Values')
plt.ylabel('Predictions')
lims = [0, 1]
plt.xlim(lims)
plt.ylim(lims)
_ = plt.plot(lims, lims)

# R2 score for test predictions
r2 = r2_score(y_test, test_pred)
print(r2)

"""## Further Enhancements

We can utilize grid search algorithm to search for the best hyper parameters for the model, and can also use k-fold cross validation to enhance the models. 
If i had more time, I would have built an ensemble model which used a number regressors as it's base model and then would have calculated the results by both averaging the predictions of all the models and also by max voting technique. Then I would have selected whichever model performed best. I would also evaluate my model more and generate more visualizations to showcase my result. 
"""